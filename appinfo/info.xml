<?xml version="1.0"?>
<info xmlns:xsi= "http://www.w3.org/2001/XMLSchema-instance"
      xsi:noNamespaceSchemaLocation="https://apps.nextcloud.com/schema/apps/info.xsd">
    <!--
    SPDX-FileCopyrightText: Marcel Klehr <mklehr@gmx.net>
    SPDX-License-Identifier: CC0-1.0
    -->
    <id>llm</id>
    <name>Local Large language model</name>
    <summary>TextProcessing provider using a Large Language Model that runs locally on CPU</summary>
    <description><![CDATA[
A TextProcessing provider Large Language Model that runs locally on CPU

The models run completely on your machine. No private data leaves your servers.

After installing this app you will need to run

    occ llm:download-models

Models:

* Llama 2 by Meta ([LLAMA 2 Community License](https://download.nextcloud.com/server/apps/llm/llama-2-7b-chat-ggml/LICENSE))
* GPT4All Falcon by Nomic AI ([Apache License 2.0](https://download.nextcloud.com/server/apps/llm/LICENSE))

Requirements:

* x86 CPU
* GNU lib C (musl is not supported)
* Python 3.10+

## Ethical AI Rating "Llama 2" model
### Rating: ðŸŸ¡

Positive:
* the software for training and inference of this model is open source
* the trained model is freely available, and thus can be run on-premises

Negative:

* the training data is not freely available, limiting the ability of external parties to check and correct for bias or optimise the modelâ€™s performance and CO2 usage.

## Ethical AI Rating for "GPT4All Falcon" model
### Rating: ðŸŸ¢

Positive:
* the software for training and inference of this model is open source
* the trained model is freely available, and thus can be run on-premises
* the training data is freely available, making it possible to check or correct for bias or optimise the performance and CO2 usage.

Learn more about the Nextcloud Ethical AI Rating [in our blog](https://nextcloud.com/blog/nextcloud-ethical-ai-rating/).

]]></description>
    <version>0.0.1</version>
    <licence>agpl</licence>
    <author mail="mklehr@gmx.net" homepage="https://marcelklehr.de">Marcel Klehr</author>
    <namespace>Llm</namespace>
    <category>tools</category>
    <bugs>https://github.com/nextcloud/llm/issues</bugs>
    <screenshot>https://raw.githubusercontent.com/nextcloud/llm/main/screenshots/Logo.png</screenshot>
    <dependencies>
        <nextcloud min-version="27.1" max-version="28"/>
    </dependencies>

    <repair-steps>
        <post-migration>
            <step>OCA\Llm\Migration\InstallDeps</step>
        </post-migration>
        <install>
            <step>OCA\Llm\Migration\InstallDeps</step>
        </install>
    </repair-steps>

    <commands>
        <command>OCA\Llm\Command\Summarize</command>
        <command>OCA\Llm\Command\DownloadModels</command>
    </commands>

    <settings>
        <admin>OCA\Llm\Settings\AdminSettings</admin>
        <admin-section>OCA\Llm\Settings\AdminSection</admin-section>
    </settings>

</info>
